{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- RAG has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. \n",
    "- When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the chunk_size. \n",
    "\n",
    "## How does one discern the optimal chunk size for seamless retrieval? \n",
    "\n",
    "- LlamaIndex Response Evaluation comes in handy. \n",
    "\n",
    "This script guides you through the steps to determine the best chunk size using LlamaIndex’s Response Evaluation module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements.txt file:\n",
    "\n",
    "# langchain-community\n",
    "# langchain-core\n",
    "# nest-asyncio\n",
    "# llama_index\n",
    "# langchain\n",
    "# llama-index-llms-openai\n",
    "# llama-index-embeddings-openai\n",
    "# ollama\n",
    "# llama-index-llms-langchain\n",
    "# llama-index-embeddings-langchain\n",
    "# spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Chunk Size Matters\n",
    "\n",
    "Choosing the right chunk_size is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
    "\n",
    "- Relevance and Granularity: \n",
    "\n",
    "    - A small chunk_size, like 128, yields more granular chunks. Risk: Vital information might not be among the top retrieved chunks, especially if the similarity_top_k setting is as restrictive as 2. \n",
    "\n",
    "    - A chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available.\n",
    "\n",
    "    ##### To navigate this, we employ the Faithfulness and Relevancy metrics. \n",
    "\n",
    "    - Measures the absence of ‘hallucinations’ & 'relevancy' of responses based on the query and the retrieved contexts respectively.\n",
    "\n",
    "- Response Generation Time: \n",
    "\n",
    "    - As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. \n",
    "    - While this can ensure a more comprehensive context, it might also slow down the system. \n",
    "    - Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
    "\n",
    "    ##### Determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. \n",
    "\n",
    "It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project_Files\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.evaluation import DatasetGenerator, FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import time\n",
    "import os\n",
    "from llama_index.core import (ServiceContext,SimpleDirectoryReader,StorageContext,VectorStoreIndex,set_global_service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='2a4682bf-941e-4895-977e-6fc1cfe7f0d8', embedding=None, metadata={'page_label': '1', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nVanka  \\nBy Anton Chekhov   \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c3f34b7d-f0ab-4749-b449-1a65edc319e6', embedding=None, metadata={'page_label': '2', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learn English Through Story  \\n \\n \\n \\n \\nVanka  \\n \\n \\n \\nBy Anton Chekhov  \\n \\nLevel 5 -6 \\n \\n \\nHope you have enjoyed the reading!  \\nCome back to https://learnenglish -new.com/ to find more \\nfascinating and exciting stories!  \\n \\n \\nhttps://learnenglish -new.com/  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f31a8025-995e-4417-9256-74c9b7ef45a6', embedding=None, metadata={'page_label': '3', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='VANKA ZHUKOV, a boy of nine, who had been for three months apprenticed to \\nAlyahin  the shoemaker, was sitting up on Christmas Eve. Waiting till his master and \\nmistress and their workmen had gone to the midnight service, he took out of his \\nmaster\\'s cupboard a bottle of ink and a pen with a rusty nib, and, spreading out a \\ncrumpled sheet o f paper in front of him, began writing. Before forming the first letter \\nhe several times looked round fearfully at the door and the windows, stole a glance at \\nthe dark ikon, on both sides of which stretched shelves full of lasts, and heaved a \\nbroken sigh. The paper lay on the bench while he knelt before it.  \\n \\n \\n \\n\"Dear grandfather, Konstantin Makaritch,\" he wrote, \"I am writing you a letter. I wish \\nyou a happy Christmas, and all blessings from God Almighty. I have neither father \\nnor mother, you are the only o ne left me.\"  \\nVanka raised his eyes to the dark ikon on which the light of his candle was reflected, \\nand vividly recalled his grandfather, Konstantin Makaritch, who was night watchman \\nto a family called Zhivarev. He was a thin but extraordinarily nimble and  lively little \\nold man of sixty -five, with an everlastingly laughing face and drunken eyes. By day \\nhe slept in the servants\\' kitchen, or made jokes with the cooks; at night, wrapped in an \\nample sheepskin, he walked round the grounds and tapped with his lit tle mallet. Old \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00decbc7-7988-4770-9d2e-294b390d9033', embedding=None, metadata={'page_label': '4', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Kashtanka and Eel, so -called on account of his dark colour and his long body like a \\nweasel\\'s, followed him with hanging heads. This Eel was exceptionally polite and \\naffectionate, and looked with equal kindness on strangers and his own maste rs, but \\nhad not a very good reputation. Under his politeness and meekness was hidden the \\nmost Jesuitical cunning. No one knew better how to creep up on occasion and snap at \\none\\'s legs, to slip into the store -room, or steal a hen from a peasant. His hind le gs had \\nbeen nearly pulled off more than once, twice he had been hanged, every week he was \\nthrashed till he was half dead, but he always revived.  \\nAt this moment grandfather was, no doubt, standing at the gate, screwing up his eyes \\nat the red windows of the church, stamping with his high felt boots, and joking with \\nthe servants. His little mallet was hanging on his belt. He was clasping his hands, \\nshrugging with the cold, and, with an aged chuckle, pinching first the housemaid, then \\nthe cook.  \\n \\n\"How about a pi nch of snuff?\" he was saying, offering the women his snuff -box. \\n \\nThe women would take a sniff and sneeze. Grandfather would be indescribably \\ndelighted, go off into a merry chuckle, and cry:  \\n \\n\"Tear it off, it has frozen on!\"  \\n \\nThey give the dogs a sniff of s nuff too. Kashtanka sneezes, wriggles her head, and \\nwalks away offended. Eel does not sneeze, from politeness, but wags his tail. And the \\nweather is glorious. The air is still, fresh, and transparent. The night is dark, but one \\ncan see the whole village wi th its white roofs and coils of smoke coming from the \\nchimneys, the trees silvered with hoar frost, the snowdrifts. The whole sky spangled \\nwith gay twinkling stars, and the Milky Way is as distinct as though it had been \\nwashed and rubbed with snow for a ho liday. . . .  \\n \\nVanka sighed, dipped his pen, and went on writing:  \\n \\n\"And yesterday I had a wigging. The master pulled me out into the yard by my hair, \\nand whacked me with a boot -stretcher because I accidentally fell asleep while I was ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f4deb17-f10c-4dbc-939b-f313046c699e', embedding=None, metadata={'page_label': '5', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='rocking their brat in t he cradle. And a week ago the mistress told me to clean a \\nherring, and I began from the tail end, and she took the herring and thrust its head in \\nmy face. The workmen laugh at me and send me to the tavern for vodka, and tell me \\nto steal the master\\'s cucumb ers for them, and the master beats me with anything that \\ncomes to hand. And there is nothing to eat. In the morning they give me bread, for \\ndinner, porridge, and in the evening, bread again; but as for tea, or soup, the master \\nand mistress gobble it all up  themselves. And I am put to sleep in the passage, and \\nwhen their wretched brat cries I get no sleep at all, but have to rock the cradle. Dear \\ngrandfather, show the divine mercy, take me away from here, home to the village. It\\'s \\nmore than I can bear. I bow  down to your feet, and will pray to God for you for ever, \\ntake me away from here or I shall die.\"  \\nVanka\\'s mouth worked, he rubbed his eyes with his black fist, and gave a sob.  \\n \\n\"I will powder your snuff for you,\" he went on. \"I will pray for you, and if I do \\nanything you can thrash me like Sidor\\'s goat. And if you think I\\'ve no job, then I will \\nbeg the steward for Christ\\'s sake to let me clean his boots, or I\\'ll go for a shepherd -\\nboy instead of Fedka. Dear grandfather, it is more than I can bear, it\\'s simply no life at \\nall. I wanted to run away to the village, but I have no boots, and I am afraid of the \\nfrost. When I grow up big I will take care of you for this, and not let anyone  annoy \\nyou, and when you die I will pray for the rest of your soul, just as for my mammy\\'s.  \\n \\nMoscow is a big town. It\\'s all gentlemen\\'s houses, and there are lots of horses, but \\nthere are no sheep, and the dogs are not spiteful. The lads here don\\'t go out with the \\nstar, and they don\\'t let anyone go into the choir, and once I saw in a shop window \\nfishing -hooks for sale, fitted ready with the line and for all sorts of fish, awfully good \\nones, there was even one hook that would hold a forty -pound sheat -fish. A nd I have \\nseen shops where there are guns of all sorts, after the pattern of the master\\'s guns at \\nhome, so that I shouldn\\'t wonder if they are a hundred roubles each. . . . And in the \\nbutchers\\' shops there are grouse and woodcocks and fish and hares, but t he shopmen \\ndon\\'t say where they shoot them.  \\n \\n\"Dear grandfather, when they have the Christmas tree at the big house, get me a gilt \\nwalnut, and put it away in the green trunk. Ask the young lady Olga Ignatyevna, say \\nit\\'s for Vanka.\"  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6f287be-9a26-4951-966a-5f04186985fc', embedding=None, metadata={'page_label': '6', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vanka gave a tremulous s igh, and again stared at the window. He remembered how \\nhis grandfather always went into the forest to get the Christmas tree for his master\\'s \\nfamily, and took his grandson with him. It was a merry time! Grandfather made a \\nnoise in his throat, the forest cr ackled with the frost, and looking at them Vanka \\nchortled too. Before chopping down the Christmas tree, grandfather would smoke a \\npipe, slowly take a pinch of snuff, and laugh at frozen Vanka. . . . The young fir trees, \\ncovered with hoar frost, stood motio nless, waiting to see which of them was to die. \\nWherever one looked, a hare flew like an arrow over the snowdrifts. . . . Grandfather \\ncould not refrain from shouting: \"Hold him, hold him . . . hold him! Ah, the bob -tailed \\ndevil!\"  \\n \\nWhen he had cut down the Christmas tree, grandfather used to drag it to the big \\nhouse, and there set to work to decorate it. . . . The young lady, who was Vanka\\'s \\nfavourite, Olga Ignatyevna, was the busiest of all. When Vanka\\'s mother Pelageya \\nwas alive, and a servant in the big h ouse, Olga Ignatyevna used to give him goodies, \\nand having nothing better to do, taught him to read and write, to count up to a \\nhundred, and even to dance a quadrille. When Pelageya died, Vanka had been \\ntransferred to the servants\\' kitchen to be with his g randfather, and from the kitchen to \\nthe shoemaker\\'s in Moscow.  \\n \\n\"Do come, dear grandfather,\" Vanka went on with his letter. \"For Christ\\'s sake, I beg \\nyou, take me away. Have pity on an unhappy orphan like me; here everyone knocks \\nme about, and I am fearful ly hungry; I can\\'t tell you what misery it is, I am always \\ncrying. And the other day the master hit me on the head with a last, so that I fell \\ndown. My life is wretched, worse than any dog\\'s. . . . I send greetings to Alyona, one -\\neyed Yegorka, and the coac hman, and don\\'t give my concertina to anyone. I remain, \\nyour grandson, Ivan Zhukov. Dear grandfather, do come.\"  \\nVanka folded the sheet of writing -paper twice, and put it into an envelope he had \\nbought the day before for a kopeck. . . . After thinking a lit tle, he dipped the pen and \\nwrote the address:  \\n \\nTo grandfather in the village.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53273c92-a0d3-4922-a46e-bd5cc9a2aa3c', embedding=None, metadata={'page_label': '7', 'file_name': 'Vanka.pdf', 'file_path': 'c:\\\\Project_Files\\\\Langchain\\\\documents\\\\short_story\\\\Vanka.pdf', 'file_type': 'application/pdf', 'file_size': 195740, 'creation_date': '2024-04-10', 'last_modified_date': '2024-04-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Then he scratched his head, thought a little, and added: Konstantin Makaritch. Glad \\nthat he had not been prevented from writing, he put on his cap and, without putting on \\nhis li ttle greatcoat, ran out into the street as he was in his shirt. . . .  \\n \\nThe shopmen at the butcher's, whom he had questioned the day before, told him that \\nletters were put in post -boxes, and from the boxes were carried about all over the \\nearth in mailcarts with drunken drivers and ringing bells. Vanka ran to the nearest \\npost-box, and thrust the precious letter in the slit. . . .  \\n \\nAn hour later, lulled by sweet hopes, he was sound asleep. . . . He dreamed of the \\nstove. On the stove was sitting his grandfather , swinging his bare legs, and reading the \\nletter to the cooks. . . .  \\n \\nBy the stove was Eel, wagging his tail.  \\n \\n \\n \\n— THE END – \\n \\n \\n \\n Hope you have enjoyed the reading! Come back to https://learnenglish -new.com/ to \\nfind more fascinating and exciting stories!  \\n \\n https://learnenglish -new.com/  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "reader = SimpleDirectoryReader(\"documents\\short_story\")\n",
    "documents = reader.load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Generation\n",
    "\n",
    "To select the right chunk_size, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various chunk_sizes. \n",
    "\n",
    "The DatasetGenerator will help us generate questions from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91984\\AppData\\Local\\Temp\\ipykernel_22136\\4172667652.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_llama2 = service_context = (ServiceContext.from_defaults(llm=llm,embed_model=embeddings,chunk_size=300))\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama model with \"llama2\" configuration.\n",
    "llm = Ollama(model=\"llama2\")\n",
    "# Initialize Ollama embeddings.\n",
    "embeddings = OllamaEmbeddings()\n",
    "service_context_llama2 = service_context = (ServiceContext.from_defaults(llm=llm,embed_model=embeddings,chunk_size=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 7/7 [00:00<00:00, 522.07it/s]\n",
      "c:\\Project_Files\\venv\\lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.evaluation.dataset_generation.DatasetGenerator at 0x1847b12a250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 4 pages.\n",
    "eval_documents = documents[:4]\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(documents, num_questions_per_chunk=2, show_progress=True, service_context=service_context_llama2)\n",
    "# dataset_generator = RagDatasetGenerator.from_documents(documents,num_questions_per_chunk=2, show_progress=True)\n",
    "\n",
    "dataset_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISSUE:\n",
    "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n",
    "\n",
    "ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at  0x000001108EBB9220>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "set OLLAMA_HOST=127.0.0.1:11433 \n",
    "\n",
    "ollama serve\n",
    "\n",
    "or \n",
    "\n",
    "netstat -ano | findstr :<PORT>\n",
    "\n",
    "taskkill /PID <PID> /F\n",
    "\n",
    "npx kill-port <PORT>\n",
    "\n",
    "#### Reason:\n",
    "\n",
    "The issue could be, check your windows services, while installing ollama as bare metal it might have installed as system service, in this case you dont need to start it as ollama serve because it might be started already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]c:\\Project_Files\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "100%|██████████| 20/20 [10:41<00:00, 32.08s/it]  \n",
      "c:\\Project_Files\\venv\\lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Great! Based on the provided context information, here are two potential questions that could be used for a quiz or examination:',\n",
       " 'What is the author\\'s name of the short story \"Vanka\"?',\n",
       " '* Context clues: The passage mentions the author\\'s name in the file name (\"By Anton Chekhov\"), and the creation date and last modified date are both on April 10, 2024.',\n",
       " 'What is the file type of the PDF document?',\n",
       " '* Context clues: The passage mentions the file type as \"application/pdf\" and provides the file size, which suggests that it is a PDF document.',\n",
       " 'Great! Based on the provided context information, here are two questions that I have generated for your upcoming quiz/examination:',\n",
       " 'What is the name of the short story being read in the passage, and who is the author? (File Name: Vanka.pdf)',\n",
       " 'When was the short story \"Vanka\" written, according to the passage? (Page Label: 2)',\n",
       " 'Of course! Based on the context information provided, here are two potential questions that could be used for a quiz or examination:',\n",
       " \"What is Vanka Zhukov's occupation according to the passage?\",\n",
       " '* Possible answer: Vanka Zhukov is an apprentice shoemaker.',\n",
       " 'What is the purpose of Vanka Zhukov writing a letter to his grandfather, Konstantin Makaritch, according to the passage?',\n",
       " \"* Possible answer: Vanka Zhukov is writing a letter to his grandfather to wish him a happy Christmas and to express his desire for his grandfather's blessings.\",\n",
       " 'Of course! Based on the context information provided, here are two questions that could be used for a quiz or examination:',\n",
       " \"What is Vanka's occupation according to the passage?\",\n",
       " 'According to the passage, what did Vanka use to write his letter to his grandfather?',\n",
       " \"Both questions require close reading of the passage and can help assess students' comprehension of the text.\",\n",
       " \"Great, let's get started! Based on the provided context information, here are two questions I would suggest for an upcoming quiz or examination:\",\n",
       " 'What is the name of the person writing the letter in the passage?',\n",
       " \"This question can help assess the students' ability to identify characters and their relationships within the text. It also allows them to make connections between the character's identity and the context of the story.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions = dataset_generator.generate_questions_from_nodes(num = 20)\n",
    "\n",
    "# eval_questions = dataset_generator.generate_dataset_from_nodes()\n",
    "# eval_questions = [\"Who is the author?, What is the main agenda of the document?, What is the title?\"]\n",
    "\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Evaluators\n",
    "\n",
    "- Setting up the llama2 model to serve as the backbone for evaluating the responses generated during the experiment. \n",
    "\n",
    "Two evaluators, FaithfulnessEvaluator and RelevancyEvaluator, are initialised with the service_context .\n",
    "\n",
    "    - Faithfulness Evaluator — Measures if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
    "    \n",
    "    - Relevancy Evaluator — Measures if the query was actually answered by the response and measures if the response + source nodes match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
    "faithfulness_llama2 = FaithfulnessEvaluator(service_context=service_context_llama2)\n",
    "relevancy_llama2 = RelevancyEvaluator(service_context=service_context_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response Evaluation For A Chunk Size\n",
    "\n",
    "We evaluate each chunk_size based on 3 metrics.\n",
    "\n",
    "    - Average Response Time.\n",
    "\n",
    "    - Average Faithfulness.\n",
    "    \n",
    "    - Average Relevancy.\n",
    "\n",
    "Here’s a function, evaluate_response_time_and_accuracy, that does just that which has:\n",
    "\n",
    "    - VectorIndex Creation.\n",
    "\n",
    "    - Building the Query Engine.\n",
    "    \n",
    "    - Metrics Calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector index\n",
    "llm_test = Ollama(model=\"mistral\")\n",
    "    \n",
    "# # Initialize Ollama embeddings.\n",
    "# embeddings = OllamaEmbeddings()\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
    "# vector_index = VectorStoreIndex.from_documents(eval_documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
    "# def evaluate_response_time_and_accuracy(chunk_size):\n",
    "#     total_response_time = 0\n",
    "#     total_faithfulness = 0\n",
    "#     total_relevancy = 0\n",
    "    \n",
    "#     service_context = ServiceContext.from_defaults(llm=llm_test, embed_model=embeddings, chunk_size=chunk_size)\n",
    "    \n",
    "#     # Set the global service context for LLAMA components.\n",
    "#     set_global_service_context(service_context)\n",
    "    \n",
    "#     # Parse nodes from the documents using the service context's node parser.\n",
    "#     nodes = (service_context.node_parser.get_nodes_from_documents(documents))\n",
    "    \n",
    "#     # Initialize storage context with default settings.\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "    \n",
    "#     # Add parsed documents (nodes) to the document store within the storage context.\n",
    "#     storage_context.docstore.add_documents(nodes)\n",
    "    \n",
    "#     # Initialize vector store index from documents, storage context, and LLAMA model.\n",
    "#     vector_index = VectorStoreIndex.from_documents(eval_documents,storage_context=storage_context,llm=llm_test)\n",
    "    \n",
    "#     query_engine = vector_index.as_query_engine(llm=llm_test)\n",
    "#     num_questions = len(eval_questions)\n",
    "    \n",
    "#     for question in eval_questions:\n",
    "#         start_time = time.time()\n",
    "#         response_vector = query_engine.query(question)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         faithfulness_result = faithfulness_llama2.evaluate_response(response=response_vector).passing\n",
    "#         relevancy_result = relevancy_llama2.evaluate_response(query=question, response=response_vector).passing\n",
    "        \n",
    "#         total_response_time += elapsed_time\n",
    "#         total_faithfulness += faithfulness_result\n",
    "#         total_relevancy += relevancy_result\n",
    "        \n",
    "#     average_response_time = total_response_time / num_questions\n",
    "#     average_faithfulness = total_faithfulness / num_questions\n",
    "#     average_relevancy = total_relevancy / num_questions\n",
    "    \n",
    "#     return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for settings: https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_response_time = 0\n",
    "total_faithfulness = 0\n",
    "total_relevancy = 0\n",
    "\n",
    "Settings.llm = llm_test\n",
    "Settings.embed_model = embeddings\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# a vector store index only needs an embed model\n",
    "index = VectorStoreIndex.from_documents(eval_documents, embed_model=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 256, average_response_time: 27.546111536026, average_faithfulness: 1.0, average_relevancy: 0.85\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm_test)\n",
    "num_questions = len(eval_questions)\n",
    "\n",
    "for question in eval_questions:\n",
    "    start_time = time.time()\n",
    "    response_vector = query_engine.query(question)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    faithfulness_result = faithfulness_llama2.evaluate_response(response=response_vector).passing\n",
    "    relevancy_result = relevancy_llama2.evaluate_response(query=question, response=response_vector).passing\n",
    "    \n",
    "    total_response_time += elapsed_time\n",
    "    total_faithfulness += faithfulness_result\n",
    "    total_relevancy += relevancy_result\n",
    "    \n",
    "average_response_time = total_response_time / num_questions\n",
    "average_faithfulness = total_faithfulness / num_questions\n",
    "average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "print(f\"Chunk size: 256, average_response_time: {average_response_time}, average_faithfulness: {average_faithfulness}, average_relevancy: {average_relevancy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_response_time = 0\n",
    "total_faithfulness = 0\n",
    "total_relevancy = 0\n",
    "\n",
    "Settings.llm = llm_test\n",
    "Settings.embed_model = embeddings\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# a vector store index only needs an embed model\n",
    "index = VectorStoreIndex.from_documents(eval_documents, embed_model=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 512, average_response_time: 25.311422169208527, average_faithfulness: 0.9, average_relevancy: 0.8\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm_test)\n",
    "num_questions = len(eval_questions)\n",
    "\n",
    "for question in eval_questions:\n",
    "    start_time = time.time()\n",
    "    response_vector = query_engine.query(question)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    faithfulness_result = faithfulness_llama2.evaluate_response(response=response_vector).passing\n",
    "    relevancy_result = relevancy_llama2.evaluate_response(query=question, response=response_vector).passing\n",
    "    \n",
    "    total_response_time += elapsed_time\n",
    "    total_faithfulness += faithfulness_result\n",
    "    total_relevancy += relevancy_result\n",
    "    \n",
    "average_response_time = total_response_time / num_questions\n",
    "average_faithfulness = total_faithfulness / num_questions\n",
    "average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "print(f\"Chunk size: 512, average_response_time: {average_response_time}, average_faithfulness: {average_faithfulness}, average_relevancy: {average_relevancy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Across Different Chunk Sizes\n",
    "\n",
    "Evaluate a range of chunk sizes to identify which offers the most promising metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
    "# for chunk_size in [256, 512]:\n",
    "#     avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n",
    "#     print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As the chunk size increases, there is a minor uptick in the Average Response Time. \n",
    "- Interestingly, the Average Faithfulness seems to reach its zenith at chunk_sizeof 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. \n",
    "- This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "- Identifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. \n",
    "- With LlamaIndex’s Response Evaluation module, you can experiment with various sizes and base your decisions on concrete data. \n",
    "- When building a RAG system, always remember that chunk_size is a pivotal parameter. \n",
    "- Invest the time to meticulously evaluate and adjust your chunk size for unmatched results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
