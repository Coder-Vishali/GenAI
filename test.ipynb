{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project_Files\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import promptquality as pq\n",
    "from promptquality import Scorers\n",
    "\n",
    "all_metrics =[\n",
    "    Scorers.latency,\n",
    "    Scorers.pii,\n",
    "    Scorers.toxicity,\n",
    "    Scorers.tone,\n",
    "    #rag metrics below\n",
    "    Scorers.context_adherence, \n",
    "    Scorers.completeness_gpt,\n",
    "    Scorers.chunk_attribution_utilization_gpt,]\n",
    "\n",
    "# Uncertainty, BLEU and ROUGE are automatically included\n",
    "\n",
    "\n",
    "#Custom scorer for response Length \n",
    "def executor(row) -> Optional[float]: \n",
    "    if row.response:\n",
    "        return len(row.response)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def aggregator(scores, indices) -> dict:\n",
    "    return {'Response Length': sum(scores)/len(scores)}\n",
    "\n",
    "length_scorer = pq.CustomScorer(name='Response Length', executor=executor, aggregator=aggregator)\n",
    "all_metrics.append(length_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3620504337.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 38\u001b[1;36m\u001b[0m\n\u001b[1;33m    rag_chain =({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()},rag prompt, llm, StrOutputParser())\u001b[0m\n\u001b[1;37m                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain. schema.runnable import RunnablePassthrough \n",
    "from langchain. schema import StrOutputParseri\n",
    "from langchain_community.vectorstores import Pinecone as langchain_pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def get_qa_chain(embeddings, index_name, k, llm_model_name, temperature):\n",
    "    # setup retriever\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    index = pc.Index(index_name)\n",
    "    vectorstore = langchain_pinecone(index, embeddings.embed_query, \"text\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # setup prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Answer the question based only on the provided context.\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Context: '{context}' \\n\\n Question: '{question}'\"\n",
    "                ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # setup LLm\n",
    "    llm = ChatOpenAI(model_name= llm_model_name, temperature=temperature)\n",
    "    \n",
    "    # helper function to format docs\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # setup chain\n",
    "    rag_chain =({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()},rag prompt, llm, StrOutputParser())\n",
    "    \n",
    "    return rag_chain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone as langchain_pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.evaluation import DatasetGenerator, FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import time\n",
    "import os\n",
    "from llama_index.core import (ServiceContext,SimpleDirectoryReader,StorageContext,VectorStoreIndex,set_global_service_context)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community import embeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "\n",
    "# Initialize Ollama embeddings.\n",
    "embeddings = OllamaEmbeddings(model=\"llama:7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model_name = \"llama\"\n",
    "dimensions= \"7b\"\n",
    "index_name = f\"{emb_model_name}-{dimensions}\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup retriever - ChromaDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=index_name,\n",
    "        embedding=embeddings.ollama.OllamaEmbeddings(model='mistral'),\n",
    "    )\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# perform the RAG \n",
    "\n",
    "# Setup prompt\n",
    "rag_template = \"\"\"Answer the question based only on the following context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "# Setup LLM\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Setup chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama embeddings.\n",
    "embeddings = OllamaEmbeddings(model=\"llama:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if our index already exists and delete stale index\n",
    "# Here I guess it is collection name\n",
    "\n",
    "# create a new index\n",
    "\n",
    "# index the documents\n",
    "\n",
    "# Load qa chain\n",
    "# Pass - embeddings, index_name, k, llm_model_name, temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "vectorstore = langchain_pinecone(index, embeddings.embed_query, \"text\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_chain_executor(emb_model_name: str, dimensions: int, llm_model_name: str, k: int) -> None:\n",
    "    # # initialise embedding model\n",
    "    # if \"text-embedding-3\" in emb_model_name:\n",
    "    #     embeddings = OpenAIEmbeddings(model=emb_model_name, \n",
    "    #                                   dimensions=dimensions)\n",
    "    # else:\n",
    "    #     embeddings = HuggingFaceEmbeddings(\n",
    "    #         model_name=emb_model_name, \n",
    "    #         encode_kwargs = {'normalize_embeddings': True})\n",
    "    \n",
    "    index_name = f\"{emb_model_name}-{dimensions}\".lower()\n",
    "\n",
    "    # First, check if our index already exists and delete stale index\n",
    "    if index_name in [index_info['name'] for index_info in pc.list_indexes()]:\n",
    "        pc.delete_index(index_name)\n",
    "        \n",
    "    # create a new index\n",
    "    pc.create_index(name=index_name, \n",
    "                    metric=\"cosine\", \n",
    "                    dimension=dimensions,\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud= \"aws\",\n",
    "                        region=\"us-west-2\"))\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    # index the documents\n",
    "    _ = langchain_pinecone.from_documents(documents, embeddings, index_name=index_name)\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Load qa chain\n",
    "    qa = get_qa_chain(embeddings, index_name, k, llm_model_name, temperature)\n",
    "    # tags to be kept in galileo run\n",
    "    run_name = f\"{index_name}\"\n",
    "    index_name_tag = pq.RunTag(key=\"Index config\", value=index_name, tag_type=pq.TagType.RAG)\n",
    "    emb_model_name_tag = pq.RunTag(key=\"Emb\", value=emb_model_name, tag_type=pq.TagType.RAG)\n",
    "    llm_model_name_tag = pq.RunTag(key=\"LLM\", value=llm_model_name, tag_type=pq.TagType.RAG)\n",
    "    dimension_tag = pq.RunTag(key=\"Dimension\", value=str(dimensions), tag_type=pq.TagType.RAG)\n",
    "    topk_tag = pq.RunTag(key=\"Top k\", value=str(k), tag_type=pq.TagType.RAG)\n",
    "\n",
    "    evaluate_handler = pq.GalileoPromptCallback(\n",
    "        project_name=project_name, \n",
    "        run_name=run_name, \n",
    "        scorers = all_metrics, \n",
    "        run_tags=[emb_model_name_tag,llm_model_name_tag,index_name_tag,dimension_tag,topk_tag]\n",
    "        )\n",
    "\n",
    "    # run chain with questions to generate the answers\n",
    "    print(\"Ready to ask!\")\n",
    "    for i, q in enumerate(tqdm(questions)):\n",
    "        print(f\"Question {i}: \", q)\n",
    "        print(qa.invoke(q, config=dict(callbacks=[evaluate_handler])))\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "    evaluate_handler. finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
